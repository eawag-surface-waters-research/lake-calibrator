{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68900a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff325ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ch1903_to_latlng(x, y):\n",
    "    x_aux = (x - 600000) / 1000000\n",
    "    y_aux = (y - 200000) / 1000000\n",
    "    lat = 16.9023892 + 3.238272 * y_aux - 0.270978 * x_aux ** 2 - 0.002528 * y_aux ** 2 - 0.0447 * x_aux ** 2 * y_aux - 0.014 * y_aux ** 3\n",
    "    lng = 2.6779094 + 4.728982 * x_aux + 0.791484 * x_aux * y_aux + 0.1306 * x_aux * y_aux ** 2 - 0.0436 * x_aux ** 3\n",
    "    lat = (lat * 100) / 36\n",
    "    lng = (lng * 100) / 36\n",
    "    return [lat, lng]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a7acd5",
   "metadata": {},
   "source": [
    "### Temperature and Conductivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814ecd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"CTD.csv\", encoding='latin1', sep=\";\")\n",
    "coords = np.array(df[\"Eastings/Northings\"])\n",
    "lat, lng = np.array([ch1903_to_latlng(float(n), float(d)) for n, d in (pair.split(' / ') for pair in coords)]).T\n",
    "df[\"latitude\"] = lat\n",
    "df[\"longitude\"] = lng\n",
    "df[\"depth\"] = df[\"Tiefe [m]\"]\n",
    "df['time'] = pd.to_datetime(df['Sampling time'], format='%d.%m.%Y %H:%M').dt.tz_localize('UTC').apply(lambda x: x.isoformat())\n",
    "df[\"weight\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "785577ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /home/runnalja/anaconda3/envs/airflow/share/proj failed\n",
      "/tmp/ipykernel_120443/3964138788.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf['geometry'] = gdf['geometry'].buffer(0.001)\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(\"lake_geometry.json\")\n",
    "gdf['geometry'] = gdf['geometry'].buffer(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deaaad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aarbergerweiher\n",
      "Ägelsee\n",
      "Arnensee\n",
      "Bachsee (Bachalpsee)\n",
      "Bachsee (unterer)\n",
      "Baggersee Hunzigen\n",
      "Baggersee Kiesen\n",
      "Baggersee Meienried\n",
      "Baggersee Münsigen\n",
      "Bannalpsee\n",
      "Biaufond\n",
      "Biessenhofer Weiher Mitte\n",
      "Birkehofweiher\n",
      "Blausee\n",
      "Bleienbacher Torfsee\n",
      "Blüemlisalpsee\n",
      "Büeltigenseelein\n",
      "Cul des Prés\n",
      "Egolzwilersee\n",
      "Elsigsee\n",
      "Engstlensee\n",
      "Fabrikweiher F.+K. Jenny AG\n",
      "Flueseeli\n",
      "Fräschelsweiher\n",
      "Gadenlauisee\n",
      "Gantrischseeli\n",
      "Göscheneralpsee\n",
      "Grosser Lauenensee\n",
      "Grossweiher\n",
      "Gütschweiher\n",
      "Hagelseewli\n",
      "Hinterburgseeli\n",
      "Hinterstockensee\n",
      "Iffigsee\n",
      "Junzlenseelein: west\n",
      "Kiesgrube Heimberg-S\n",
      "Kwatt Weiher\n",
      "La Marnière\n",
      "La Noz\n",
      "Lac de Nervaux\n",
      "Lac de Ter\n",
      "Lac des Taillers\n",
      "Lac Vert\n",
      "Lauenensee: kleiner\n",
      "Les Chaufours\n",
      "Lütscheren: westlich\n",
      "Meienfallseeli\n",
      "Melchsee\n",
      "Muemethaler Weier\n",
      "Muggeseeli\n",
      "Mühliguetweiher\n",
      "Näelser: Obersee\n",
      "Oberalpsee\n",
      "Oberer Chatzensee\n",
      "Oberstockensee\n",
      "Remersee\n",
      "Rezligletschersee\n",
      "Sängeliweiher\n",
      "Schiffenensee\n",
      "Schwarzsee\n",
      "Seebergsee\n",
      "Seebodensee\n",
      "Seelisbergsee\n",
      "Sgistalsee\n",
      "Siselenweiher\n",
      "Spittelmattesee\n",
      "Steinibühlweiher\n",
      "Sulssewli\n",
      "Sulssewli: oberes\n",
      "Tälliseeli\n",
      "Tannensee\n",
      "Triebtenseewli\n",
      "Trübsee\n",
      "Tuetenseeli\n",
      "Unterer Bommerweiher\n",
      "Vierwaldstättersee: Alpnachersee\n",
      "Walopsee\n",
      "Walopsee: hinterer\n",
      "Wannisbordsee\n",
      "Weiher bei Hagneck\n",
      "Weiher bei Hardern\n",
      "Widi\n",
      "Wilersee\n",
      "Wyssensee\n",
      "Atzenholzweiher\n",
      "Bildweiher\n",
      "Gräppelensee\n",
      "Eselschwanzweiher\n",
      "Schlossweiher\n",
      "Schwendisee\n",
      "Stadtweiher Wil\n",
      "Voralpsee\n",
      "Wenigerweiher\n",
      "Wichenstein Gross\n",
      "Wichenstein Klein\n",
      "Biessenhofer Weiher Ost\n",
      "Biessenhofer Weiher West\n",
      "Horbacher Weiher\n",
      "Horber Weiher\n",
      "Lago del Starlaresc da Sgiof\n",
      "Lago di  Morghirolo\n",
      "Lago Nero\n",
      "Lago della Cròsa Superiore\n",
      "Lago d'Alzasca\n",
      "LAC MON-4 / 1349\n",
      "LAC PER-4a / 1354\n",
      "LAC LES-4 / 1348\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "failed = {}\n",
    "for index, row in df.iterrows():\n",
    "    point = Point(row[\"longitude\"], row[\"latitude\"])\n",
    "    is_inside = gdf.contains(point)\n",
    "    if is_inside.any():\n",
    "        polygons_containing_point = gdf[is_inside]\n",
    "        if len(polygons_containing_point) != 1:\n",
    "            print(\"Overlapping polygons\")\n",
    "            print(polygons_containing_point)\n",
    "        else:\n",
    "            lake = polygons_containing_point.iloc[0][\"key\"]\n",
    "            if lake in results:\n",
    "                results[lake].append(index)\n",
    "            else:\n",
    "                results[lake] = [index]\n",
    "    else:\n",
    "        if row[\"Water body\"] not in failed:\n",
    "            print(row[\"Water body\"])\n",
    "            failed[row[\"Water body\"]] = 0\n",
    "        else:\n",
    "            failed[row[\"Water body\"]] = failed[row[\"Water body\"]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6fc6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"value\"] = df[\"Wassertemperatur / T-Wasser [°C]\"]\n",
    "df_t = df[[\"time\", \"depth\", \"latitude\", \"longitude\", \"value\", \"weight\"]]\n",
    "\n",
    "for lake in results.keys():\n",
    "    df_s = df_t.iloc[results[lake]].dropna(subset=['value'])\n",
    "    os.makedirs('../observations/{}'.format(lake), exist_ok=True)\n",
    "    df_s.to_csv('../observations/{}/temperature.csv'.format(lake), index=False)\n",
    "    \n",
    "df[\"value\"] = df[\"el_LF / Elektrische Leitfähigkeit [µS/cm]\"]\n",
    "df_t = df[[\"time\", \"depth\", \"latitude\", \"longitude\", \"value\", \"weight\"]]\n",
    "\n",
    "for lake in results.keys():\n",
    "    df_s = df_t.iloc[results[lake]].dropna(subset=['value'])\n",
    "    os.makedirs('../observations/{}'.format(lake), exist_ok=True)\n",
    "    df_s.to_csv('../observations/{}/conductivity.csv'.format(lake), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8d74f",
   "metadata": {},
   "source": [
    "### Secchi Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a961d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Secchi.csv\", encoding='latin1', sep=\";\")\n",
    "lat, lng = ch1903_to_latlng(np.array(df[\"X [m]\"]),np.array(df[\"Y [m]\"]))\n",
    "df[\"latitude\"] = lat\n",
    "df[\"longitude\"] = lng\n",
    "df[\"depth\"] = None\n",
    "df['time'] = pd.to_datetime(df['Time'], format='%d.%m.%Y %H:%M').dt.tz_localize('UTC').apply(lambda x: x.isoformat())\n",
    "df[\"value\"] = df[\"Secchi depth [m]\"]\n",
    "df[\"weight\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a31acf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SÃ¤ngeliweiher\n",
      "oberer Katzensee\n",
      "La Noz\n",
      "Schiffenensee\n",
      "Biessenhofer Weiher Mitte\n",
      "Kwatt Weiher\n",
      "Atzenholzweiher\n",
      "Bildweiher\n",
      "GrÃ¤ppelensee\n",
      "Eselschwanzweiher\n",
      "Schlossweiher\n",
      "Schwendisee\n",
      "Stadtweiher Wil\n",
      "Voralpsee\n",
      "Wenigerweiher\n",
      "Wichenstein Gross\n",
      "Wichenstein Klein\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "failed = {}\n",
    "for index, row in df.iterrows():\n",
    "    point = Point(row[\"longitude\"], row[\"latitude\"])\n",
    "    is_inside = gdf.contains(point)\n",
    "    if is_inside.any():\n",
    "        polygons_containing_point = gdf[is_inside]\n",
    "        if len(polygons_containing_point) != 1:\n",
    "            print(\"Overlapping polygons\")\n",
    "            print(polygons_containing_point)\n",
    "        else:\n",
    "            lake = polygons_containing_point.iloc[0][\"key\"]\n",
    "            if lake in results:\n",
    "                results[lake].append(index)\n",
    "            else:\n",
    "                results[lake] = [index]\n",
    "    else:\n",
    "        if row[\"Lake\"] not in failed:\n",
    "            print(row[\"Lake\"])\n",
    "            failed[row[\"Lake\"]] = 0\n",
    "        else:\n",
    "            failed[row[\"Lake\"]] = failed[row[\"Lake\"]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4454fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df[[\"time\", \"depth\", \"latitude\", \"longitude\", \"value\", \"weight\"]]\n",
    "\n",
    "for lake in results.keys():\n",
    "    df_s = df_t.iloc[results[lake]].dropna(subset=['value'])\n",
    "    os.makedirs('../observations/{}'.format(lake), exist_ok=True)\n",
    "    df_s.to_csv('../observations/{}/secchi.csv'.format(lake), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032009e5",
   "metadata": {},
   "source": [
    "### Ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e8f91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Ice.csv\", encoding='latin1', sep=\";\")\n",
    "lat, lng = ch1903_to_latlng(np.array(df[\"X [m]\"]),np.array(df[\"Y [m]\"]))\n",
    "df[\"latitude\"] = lat\n",
    "df[\"longitude\"] = lng\n",
    "df[\"depth\"] = None\n",
    "df['time'] = pd.to_datetime(df['Datum'], format='%d.%m.%Y %H:%M').dt.tz_localize('UTC').apply(lambda x: x.isoformat())\n",
    "df[\"value\"] = df[\"Ice thickness [m]\"]\n",
    "df[\"weight\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c4c82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "failed = {}\n",
    "for index, row in df.iterrows():\n",
    "    point = Point(row[\"longitude\"], row[\"latitude\"])\n",
    "    is_inside = gdf.contains(point)\n",
    "    if is_inside.any():\n",
    "        polygons_containing_point = gdf[is_inside]\n",
    "        if len(polygons_containing_point) != 1:\n",
    "            print(\"Overlapping polygons\")\n",
    "            print(polygons_containing_point)\n",
    "        else:\n",
    "            lake = polygons_containing_point.iloc[0][\"key\"]\n",
    "            if lake in results:\n",
    "                results[lake].append(index)\n",
    "            else:\n",
    "                results[lake] = [index]\n",
    "    else:\n",
    "        if row[\"Lake\"] not in failed:\n",
    "            print(row[\"Lake\"])\n",
    "            failed[row[\"Lake\"]] = 0\n",
    "        else:\n",
    "            failed[row[\"Lake\"]] = failed[row[\"Lake\"]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a5a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df[[\"time\", \"depth\", \"latitude\", \"longitude\", \"value\", \"weight\"]]\n",
    "\n",
    "for lake in results.keys():\n",
    "    df_s = df_t.iloc[results[lake]].dropna(subset=['value'])\n",
    "    os.makedirs('../observations/{}'.format(lake), exist_ok=True)\n",
    "    df_s.to_csv('../observations/{}/ice.csv'.format(lake), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a2f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "airflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
